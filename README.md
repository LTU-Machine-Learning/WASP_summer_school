# WASP thematic summer school 2025 on "Rethinking and Rescaling LLMs"

This project provides examples on Knowledge Distillation (rationale-based method - incorporates step-by-step reasoning) and Parameter-efficient Finetuning (using QLoRA).

References for this repo are thanks to https://github.com/anaumghori/step-distillation and https://github.com/google-research/distilling-step-by-step 
