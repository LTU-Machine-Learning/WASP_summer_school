# WASP thematic summer school 2025 on "Rethinking and Rescaling LLMs"

This project provides examples on Knowledge Distillation (rationale-based method - incorporates step-by-step reasoning) and Parameter-efficient Finetuning (using QLoRA).

References for KD are thanks to <a href="https://github.com/anaumghori/step-distillation">Anaumghori</a> and <a ref="https://github.com/google-research/distilling-step-by-step">Google Research</a>. Reference for QLoRA is thanks to <a href="https://medium.com/@rschaeffer23/how-to-fine-tune-llama-3-1-8b-instruct-bf0a84af7795">Schaeffer23</a>.
