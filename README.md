# WASP thematic summer school 2025 on "Rethinking and Rescaling LLMs"

This project provides examples on Knowledge Distillation (rationale-based method - incorporates step-by-step reasoning) and Parameter-efficient Finetuning (using QLoRA).

References for KD are thanks to https://github.com/anaumghori/step-distillation and https://github.com/google-research/distilling-step-by-step 

Reference for QLoRA is thanks to https://medium.com/@rschaeffer23/how-to-fine-tune-llama-3-1-8b-instruct-bf0a84af7795 
